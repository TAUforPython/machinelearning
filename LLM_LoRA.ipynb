{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a6ef64ece37421c86b384144f0138da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7ba10eed371478484b6a5c76abb7fee",
              "IPY_MODEL_f477eafec74e4002ad30a94a62f33833",
              "IPY_MODEL_b16acccdcc964f569aaacde5d648b7f1"
            ],
            "layout": "IPY_MODEL_3785f82368804d5f9bcdbd13354d904a"
          }
        },
        "c7ba10eed371478484b6a5c76abb7fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7ed5a07b08b4aafb27de6f30ae3444b",
            "placeholder": "​",
            "style": "IPY_MODEL_f43b30e4ca6a484cb3c1e17e85b41ca0",
            "value": "Map: 100%"
          }
        },
        "f477eafec74e4002ad30a94a62f33833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cef4de3e599e43b09133eb096cbbcdc7",
            "max": 130319,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a474e76aa4d4293888b827923f96a69",
            "value": 130319
          }
        },
        "b16acccdcc964f569aaacde5d648b7f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3e173cf0552421ca6c8b5fb7991a10b",
            "placeholder": "​",
            "style": "IPY_MODEL_8b20ec72717b418b9fd7a04e5c3c0274",
            "value": " 130319/130319 [02:16&lt;00:00, 906.50 examples/s]"
          }
        },
        "3785f82368804d5f9bcdbd13354d904a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7ed5a07b08b4aafb27de6f30ae3444b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f43b30e4ca6a484cb3c1e17e85b41ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cef4de3e599e43b09133eb096cbbcdc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a474e76aa4d4293888b827923f96a69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3e173cf0552421ca6c8b5fb7991a10b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b20ec72717b418b9fd7a04e5c3c0274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80cd4cfd4adf43c9b3bc43c0a5267ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de04c56cd73d483e953201143c1f6768",
              "IPY_MODEL_6ad1d9a29fd64541905da2d532c562b9",
              "IPY_MODEL_af3fce939d3745249d1737151f3f1c9d"
            ],
            "layout": "IPY_MODEL_839fd68849944de4b82186d8de140bb1"
          }
        },
        "de04c56cd73d483e953201143c1f6768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a66c08cb099a403f9109f084373c8bfa",
            "placeholder": "​",
            "style": "IPY_MODEL_3b45fce768534e1787128c840dd2c17c",
            "value": "Map: 100%"
          }
        },
        "6ad1d9a29fd64541905da2d532c562b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc59a0ee273546a3bc1794b108149244",
            "max": 11873,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f4d457bd97e438284b1dce2e41486d1",
            "value": 11873
          }
        },
        "af3fce939d3745249d1737151f3f1c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec63a07a1dee467ea01de4c154287b7e",
            "placeholder": "​",
            "style": "IPY_MODEL_11d2a66530c648ab8ec08f041e514448",
            "value": " 11873/11873 [00:13&lt;00:00, 1033.12 examples/s]"
          }
        },
        "839fd68849944de4b82186d8de140bb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a66c08cb099a403f9109f084373c8bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b45fce768534e1787128c840dd2c17c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc59a0ee273546a3bc1794b108149244": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f4d457bd97e438284b1dce2e41486d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec63a07a1dee467ea01de4c154287b7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11d2a66530c648ab8ec08f041e514448": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAUforPython/machinelearning/blob/for-example-and-testing/LLM_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA\n",
        "based on a combination of this:\n",
        "https://huggingface.co/docs/peft/task_guides/semantic_segmentation_lora\n",
        "and this:\n",
        "\n",
        "https://www.youtube.com/watch?v=iYr1xZn26R8\n",
        "\n",
        "https://github.com/huggingface/peft/issues/493\n",
        "\n",
        "\n",
        "Recommended runtime: v100 high RAM. A100 high RAM if you use a larger BLOOM model.\n",
        "\n",
        "Note, I generally prioritize ease of comparing a model and it's fine tuned counterpart over inference time.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Z9s6u5X9z_uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Dependencies\n",
        "- **bitsandbytes:** for representing models using smaller datatypes, saving on memory.\n",
        "- **datasets:** for downloading datasets\n",
        "- **accelerate:** required dependency for machine learning interoperability\n",
        "- **loralib:** LoRA implementation\n",
        "- **peft:** a general \"parameter efficient fine tuning\" module, our interface for LoRA\n",
        "- **transformers:** for downloading and using pre-trained transformers from huggingface."
      ],
      "metadata": {
        "id": "01zIxuZX0mhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7aYz5D70CCg",
        "outputId": "26bd9056-c65e-4217-fdb5-010e5c80b204"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Pre-Trained Model\n",
        "in this model we're using the BLOOM, decoder only, causal language model. This is a permissively source language model trained on a variety of data.\n",
        "\n",
        "We'll be using the 560m parameter version to save on GPU memory, but if you use an A100 instance you should be able to run the 3b parameter version. While not thoroughly tested, all code should work for any flavor of BLOOM"
      ],
      "metadata": {
        "id": "xUGgwzqK124J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Большие языковые модели, такие как GPT, Mistral, LLaMA и другие, содержат миллионы или миллиарды параметров. Полное переобучение таких моделей на новых данных требует огромных вычислительных ресурсов. LoRA решает эту проблему, позволяя адаптировать модель, изменяя только небольшую часть её параметров, что значительно снижает затраты на вычисления и память.\n",
        "\n",
        "Как работает LoRA?\n",
        "\n",
        "LoRA основан на идее низкорангового разложения (low-rank decomposition). Вместо того чтобы обновлять все параметры модели, LoRA добавляет к весам модели небольшие низкоранговые матрицы, которые обучаются на специфических данных. Основная модель остаётся замороженной (неизменной), а адаптация происходит через эти дополнительные матрицы.\n",
        "\n"
      ],
      "metadata": {
        "id": "NYL_mp7wpqJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Importing dependencies and downloading pre-trained bloom model\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "#loading model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    # \"bigscience/bloom-3b\",\n",
        "    # \"bigscience/bloom-1b1\",\n",
        "    \"bigscience/bloom-560m\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "#loading tokenizer for this model (which turns text into an input for the model)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/tokenizer\")"
      ],
      "metadata": {
        "id": "34LkV4O514v-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up LoRA\n",
        "- **r:** the rank of the A and B matrices\n",
        "- **lora_alpha:** this is a pretty controversial parameter. A lot of people hava a lot of ideas about it. You can consider it a scaling factor, and by default it should be equal to `r`, as far as I understand.\n",
        "- **target_modules:** the portions of the model we want to optimize with LoRA. the BLOOM module has parameters named `query_key_value` which we want to optimize.\n",
        "- **lora_dropout:** dropout is a technique which hides inputs to suppress the model from overfitting (called regularization). This is a probability of being hidden.\n",
        "- **bias:** neural networks typically have two paramet per connection, a \"weight\" and a \"bias\". We're only training weights in this example.\n",
        "- **task_type:** not super necessary, used in the superclass `PeftConfig`. Setting to `CAUSAL_LM` because the specific language model we're using is \"causal\"."
      ],
      "metadata": {
        "id": "5bFZ2oKI2fsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Setting up LoRA using parameter efficient fine tuning\n",
        "\"\"\"\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "#defining how LoRA will work in this particular example\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "#this actually overwrites the model in memory, so\n",
        "#the rename is only for ledgibility.\n",
        "peft_model = get_peft_model(model, config)\n"
      ],
      "metadata": {
        "id": "BXq6vYyb29yQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Printing Trainable Parameter Difference"
      ],
      "metadata": {
        "id": "qsSod_or3Fax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Comparing parameters before and after LoRA\n",
        "\"\"\"\n",
        "\n",
        "trainable_params = 0\n",
        "all_param = 0\n",
        "\n",
        "#iterating over all parameters\n",
        "for _, param in peft_model.named_parameters():\n",
        "    #adding parameters to total\n",
        "    all_param += param.numel()\n",
        "    #adding parameters to trainable if they require a graident\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "\n",
        "#printing results\n",
        "print(f\"trainable params: {trainable_params}\")\n",
        "print(f\"all params: {all_param}\")\n",
        "print(f\"trainable: {100 * trainable_params / all_param:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ep1kImT3KJc",
        "outputId": "0c5964e4-b061-415d-eccf-dcb5aae07cef"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 786432\n",
            "all params: 560001024\n",
            "trainable: 0.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset\n",
        "this is the stanford question answering dataset (SQUAD), which we'll use to fine tune BLOOM to improve performance on question answering.\n",
        "\n",
        "датасет SQuAD 2.0 (Stanford Question Answering Dataset) из библиотеки Hugging Face Datasets."
      ],
      "metadata": {
        "id": "BCii0BM441AF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Loading SQUAD dataset\n",
        "\"\"\"\n",
        "\n",
        "from datasets import load_dataset\n",
        "qa_dataset = load_dataset(\"squad_v2\")"
      ],
      "metadata": {
        "id": "GPbR9Yk132nU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-Formatting\n",
        "We're going to get the LLM to learn a specific format (a common use of fine tuning).\n",
        "\n",
        "```\n",
        "**CONTEXT:**\n",
        "{context}\n",
        "\n",
        "**QUESTION:**\n",
        "{question}\n",
        "\n",
        "**ANSWER:**\n",
        "{answer}</s>\n",
        "```\n",
        "\n",
        "So, we'll reformat our SQUAD dataset to respect that format."
      ],
      "metadata": {
        "id": "0qqx4lwA5x9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Reformatting SQUAD to respect our defined structure\n",
        "\"\"\"\n",
        "\n",
        "#defining a function for reformatting\n",
        "def create_prompt(context, question, answer):\n",
        "  if len(answer[\"text\"]) < 1:\n",
        "    answer = \"Cannot Find Answer\"\n",
        "  else:\n",
        "    answer = answer[\"text\"][0]\n",
        "  prompt_template = f\"CONTEXT:\\n{context}\\n\\nQUESTION:\\n{question}\\n\\nANSWER:\\n{answer}</s>\"\n",
        "  return prompt_template\n",
        "\n",
        "#applying the reformatting function to the entire dataset\n",
        "mapped_qa_dataset = qa_dataset.map(lambda samples: tokenizer(create_prompt(samples['context'], samples['question'], samples['answers'])))"
      ],
      "metadata": {
        "id": "gpDfILFX6Tud",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "3a6ef64ece37421c86b384144f0138da",
            "c7ba10eed371478484b6a5c76abb7fee",
            "f477eafec74e4002ad30a94a62f33833",
            "b16acccdcc964f569aaacde5d648b7f1",
            "3785f82368804d5f9bcdbd13354d904a",
            "a7ed5a07b08b4aafb27de6f30ae3444b",
            "f43b30e4ca6a484cb3c1e17e85b41ca0",
            "cef4de3e599e43b09133eb096cbbcdc7",
            "7a474e76aa4d4293888b827923f96a69",
            "f3e173cf0552421ca6c8b5fb7991a10b",
            "8b20ec72717b418b9fd7a04e5c3c0274",
            "80cd4cfd4adf43c9b3bc43c0a5267ae0",
            "de04c56cd73d483e953201143c1f6768",
            "6ad1d9a29fd64541905da2d532c562b9",
            "af3fce939d3745249d1737151f3f1c9d",
            "839fd68849944de4b82186d8de140bb1",
            "a66c08cb099a403f9109f084373c8bfa",
            "3b45fce768534e1787128c840dd2c17c",
            "fc59a0ee273546a3bc1794b108149244",
            "4f4d457bd97e438284b1dce2e41486d1",
            "ec63a07a1dee467ea01de4c154287b7e",
            "11d2a66530c648ab8ec08f041e514448"
          ]
        },
        "outputId": "baa59973-ae48-4697-ad1f-ade73f266914"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/130319 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a6ef64ece37421c86b384144f0138da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80cd4cfd4adf43c9b3bc43c0a5267ae0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training our LoRA model on SQUAD\n",
        "Updating the decomposed matrices to improve the model on question answering, and teach it the desired structure."
      ],
      "metadata": {
        "id": "ytep61CF6lQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Fine Tuning\n",
        "This code is largly co-opted. In the absence of a rigid validation\n",
        "procedure, the best practice is to just copy a successful tutorial or,\n",
        "better yet, directly from the documentation.\n",
        "\"\"\"\n",
        "\n",
        "import transformers\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=mapped_qa_dataset[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        max_steps=10,\n",
        "        learning_rate=1e-3,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs',\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "#peft_model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "bVcKf0h16wL-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "72dbfc15-bf11-43e7-e6a6-f71e3fc7a98c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:12, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 13.5514, 'train_samples_per_second': 11.807, 'train_steps_per_second': 0.738, 'total_flos': 75064509825024.0, 'train_loss': 0.0, 'epoch': 0.0012277470841006752})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Locally\n",
        "saving our LoRA fine tune results."
      ],
      "metadata": {
        "id": "7ki4RGWm7UGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Saving the LoRA fine tuning locally\n",
        "\"\"\"\n",
        "model_id = \"BLOOM-560m-LoRA\"\n",
        "peft_model.save_pretrained(model_id)"
      ],
      "metadata": {
        "id": "I-tQ5ng27ts0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking File Size\n",
        "Compare this to the size of the initial model download to get an idea of the memory savings."
      ],
      "metadata": {
        "id": "eT2bM7ou76Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh {model_id}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-pnuRL48F1S",
        "outputId": "cd9f8b06-4c3f-4669-eecb-5751f1e25820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 3.1M\n",
            "-rw-r--r-- 1 root root  482 Nov  6 14:17 adapter_config.json\n",
            "-rw-r--r-- 1 root root 3.1M Nov  6 14:17 adapter_model.bin\n",
            "-rw-r--r-- 1 root root 5.3K Nov  6 14:17 README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "uGBiNfNbm98k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Helper Function for Comparing Results\n",
        "\"\"\"\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def make_inference(context, question):\n",
        "\n",
        "    #turn the input into tokens\n",
        "    batch = tokenizer(f\"**CONTEXT:**\\n{context}\\n\\n**QUESTION:**\\n{question}\\n\\n**ANSWER:**\\n\", return_tensors='pt', return_token_type_ids=False)\n",
        "    #move the tokens onto the GPU, for inference\n",
        "    batch = batch.to(device='cuda')\n",
        "\n",
        "    #make an inference with both the fine tuned model and the raw model\n",
        "    with torch.cuda.amp.autocast():\n",
        "        #I think inference time would be faster if these were applied,\n",
        "        #but the fact that LoRA is not applied allows me to experiment\n",
        "        #with before and after fine tuning simultaniously\n",
        "\n",
        "        #raw model\n",
        "        peft_model.disable_adapter_layers()\n",
        "        output_tokens_raw = model.generate(**batch, max_new_tokens=200)\n",
        "\n",
        "        #LoRA model\n",
        "        peft_model.enable_adapter_layers()\n",
        "        output_tokens_qa = peft_model.generate(**batch, max_new_tokens=200)\n",
        "\n",
        "    #display results\n",
        "    display(Markdown(\"# Raw Model\\n\"))\n",
        "    display(Markdown((tokenizer.decode(output_tokens_raw[0], skip_special_tokens=True))))\n",
        "    display(Markdown(\"\\n# QA Model\\n\"))\n",
        "    display(Markdown((tokenizer.decode(output_tokens_qa[0], skip_special_tokens=True))))"
      ],
      "metadata": {
        "id": "vSZZjyp1mPoJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"you are a math wizard\"\n",
        "question = \"what is 1+1 equal to?\"\n",
        "\n",
        "make_inference(context, question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "id": "OrFE2IvG_B1I",
        "outputId": "08eb6a53-db8c-4cc7-f29c-8a02676cf90c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-3fa639ed891c>:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Raw Model\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**CONTEXT:**\nyou are a math wizard\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n# QA Model\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**CONTEXT:**\nyou are a math wizard\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal to 1.0\n\n**QUESTION:**\nwhat is 1+1 equal to?\n\n**ANSWER:**\n1+1 is equal"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Answer the riddle\"\n",
        "question = \"What gets bigger the more you take away?\"\n",
        "\n",
        "make_inference(context, question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "XNKu-ugS_0wV",
        "outputId": "644c73d0-90a2-4ba0-9fe6-03589e2bc807"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-3fa639ed891c>:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Raw Model\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**CONTEXT:**\nAnswer the riddle\n\n**QUESTION:**\nWhat gets bigger the more you take away?\n\n**ANSWER:**\nThe answer is that the more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n# QA Model\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**CONTEXT:**\nAnswer the riddle\n\n**QUESTION:**\nWhat gets bigger the more you take away?\n\n**ANSWER:**\nThe answer is that the more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The more you take away, the more you get away from the problem. The"
          },
          "metadata": {}
        }
      ]
    }
  ]
}