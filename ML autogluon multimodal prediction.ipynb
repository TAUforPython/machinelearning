{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvHGLo+lnzv+PpP5jP0cdH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAUforPython/machinelearning/blob/main/ML%20autogluon%20multimodal%20prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install AutoGluon (Run this first)\n",
        "# This cell installs AutoGluon and its multimodal components.\n",
        "# It might take a few minutes.\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Install AutoGluon Multimodal (This includes core and tabular dependencies)\n",
        "# Using --no-deps can sometimes help avoid dependency conflicts,\n",
        "# but AutoGluon usually manages its dependencies well.\n",
        "# The --quiet flag reduces output noise during installation.\n",
        "!pip install autogluon.multimodal --quiet"
      ],
      "metadata": {
        "id": "KLE7k7CMr2yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# It's good practice to restart the runtime after installation\n",
        "# to ensure all packages are loaded correctly.\n",
        "# print(\"Please restart the runtime after installation completes.\")\n",
        "# print(\"Go to Runtime -> Restart Runtime in Colab menu.\")\n",
        "# sys.exit() # Uncomment if you want the cell to explicitly stop here.\n",
        "\n",
        "# After restart, you can run the rest of the code.\n",
        "print(\"AutoGluon Multimodal installed successfully. You may need to restart the runtime.\")\n",
        "\n",
        "# %%\n",
        "# @title Import Libraries and Set Seeds\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from autogluon.multimodal import MultiModalPredictor\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(123)\n",
        "print(\"Libraries imported and seeds set.\")"
      ],
      "metadata": {
        "id": "IqnW6sJkr7OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation (train_data and test_data):\n",
        "The train_data DataFrame (and test_data) contains multiple columns.\n",
        "\n",
        "**Tabular Data**: Columns like Type, Age, Breed1, Breed2, Gender, Color1, Color2, Color3, MaturitySize, FurLength, Vaccinated, Dewormed, Sterilized, Health, Quantity, Fee, State, VideoAmt, PhotoAmt are numerical or categorical values. AutoGluon treats these as tabular features.\n",
        "\n",
        "**Text Data:** The Description column contains free-form text descriptions of the pets. AutoGluon identifies this column as text data and uses text processing models (like transformers) to handle it.\n",
        "\n",
        "**Image Data:** The Images column contains file paths (or potentially URLs) pointing to the actual image files. AutoGluon recognizes these paths and uses computer vision models (like CNNs) to process the image content."
      ],
      "metadata": {
        "id": "Mb7zYevFz-n5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download and Prepare the PetFinder Dataset\n",
        "# This dataset contains images, text descriptions, and tabular features\n",
        "# for pets, with the goal of predicting adoption speed.\n",
        "\n",
        "download_dir = './ag_automm_tutorial'\n",
        "zip_file = 'https://automl-mm-bench.s3.amazonaws.com/petfinder_for_tutorial.zip'\n",
        "\n",
        "# Download and extract the dataset\n",
        "from autogluon.core.utils.loaders import load_zip\n",
        "load_zip.unzip(zip_file, unzip_dir=download_dir)\n",
        "\n",
        "# Load the training and testing data\n",
        "dataset_path = os.path.join(download_dir, 'petfinder_for_tutorial')\n",
        "train_data = pd.read_csv(os.path.join(dataset_path, 'train.csv'), index_col=0)\n",
        "test_data = pd.read_csv(os.path.join(dataset_path, 'test.csv'), index_col=0)\n",
        "\n",
        "label_col = 'AdoptionSpeed' # The column to predict\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n",
        "print(f\"Label column: {label_col}\")"
      ],
      "metadata": {
        "id": "nZkpfLKWsBvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# @title Expand Image Paths\n",
        "# The image paths in the CSV are relative; we need absolute paths for the model to read them.\n",
        "image_col = 'Images'\n",
        "\n",
        "# Use only the first image if multiple are listed per pet\n",
        "train_data[image_col] = train_data[image_col].apply(lambda ele: ele.split(';')[0] if ele and ';' in ele else ele)\n",
        "test_data[image_col] = test_data[image_col].apply(lambda ele: ele.split(';')[0] if ele and ';' in ele else ele)\n",
        "\n",
        "def path_expander(path, base_folder):\n",
        "    # Ensure the path is an absolute path relative to the base folder\n",
        "    if pd.isna(path) or path == '':\n",
        "        return path # Handle missing paths gracefully if necessary\n",
        "    return os.path.abspath(os.path.join(base_folder, path))\n",
        "\n",
        "train_data[image_col] = train_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n",
        "test_data[image_col] = test_data[image_col].apply(lambda ele: path_expander(ele, base_folder=dataset_path))\n",
        "\n",
        "print(f\"First training image path: {train_data[image_col].iloc[0]}\")\n",
        "print(f\"First test image path: {test_data[image_col].iloc[0]}\")"
      ],
      "metadata": {
        "id": "fsEpIx0bsFz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# @title Inspect Sample Data\n",
        "# Let's look at an example row to understand the data structure.\n",
        "example_row = train_data.iloc[2]\n",
        "print(\"\\n--- Example Training Row ---\")\n",
        "print(example_row)\n",
        "print(\"\\n--- Text Description for Example Row ---\")\n",
        "print(example_row['Description'])\n",
        "\n",
        "# Optional: Display the image (if available)\n",
        "try:\n",
        "    from IPython.display import Image as IPImage, display\n",
        "    example_image_path = example_row[image_col]\n",
        "    if os.path.exists(example_image_path):\n",
        "        print(\"\\n--- Image for Example Row ---\")\n",
        "        display(IPImage(filename=example_image_path, width=200, height=200))\n",
        "    else:\n",
        "        print(f\"\\n--- Image file not found: {example_image_path} ---\")\n",
        "except ImportError:\n",
        "    print(\"IPython not available for image display in this environment.\")"
      ],
      "metadata": {
        "id": "KyBLurtysIgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# @title Define Hyperparameters (Optional) - Using Default Behavior\n",
        "# The MultiModalPredictor often works well with default settings.\n",
        "# You can specify hyperparameters directly in the fit method if needed,\n",
        "# but the presets functionality might be integrated differently now.\n",
        "# For example, you can pass arguments like presets=\"medium_quality\" directly to fit().\n",
        "\n",
        "# Define the predictor\n",
        "predictor = MultiModalPredictor(label=label_col)\n",
        "\n",
        "# Fit the model. Adjust time_limit based on your needs and computational resources.\n",
        "# A higher time limit often leads to better performance.\n",
        "# Using presets=\"medium_quality\" directly in fit()\n",
        "fit_results = predictor.fit(\n",
        "    train_data=train_data,\n",
        "    time_limit=120, # Train for 60 seconds as an example\n",
        "    presets=\"medium_quality\", # Use the preset directly here\n",
        "    # You can add other arguments like tuning_data for validation split if needed\n",
        ")\n",
        "print(\"\\nTraining completed.\")\n",
        "# The fit_results might contain information, but the primary model is stored in 'predictor'\n",
        "# print(f\"Fit results: {fit_results}\") # This might not always contain a 'best_score' key\n",
        "\n",
        "# %%\n",
        "# @title Evaluate the Model on Test Data\n",
        "# Evaluate the trained model's performance.\n",
        "# Common metrics for classification include 'accuracy', 'roc_auc', 'f1', 'precision', 'recall'.\n",
        "# Note: 'roc_auc' might require prediction probabilities for binary classification.\n",
        "scores = predictor.evaluate(test_data, metrics=['accuracy', 'roc_auc'])\n",
        "print(\"\\n--- Evaluation Results on Test Data ---\")\n",
        "print(scores)\n",
        "\n",
        "# %%\n",
        "# @title Make Predictions on Test Data (Without Labels)\n",
        "# Predict the AdoptionSpeed for the test set.\n",
        "test_features = test_data.drop(columns=[label_col])\n",
        "predictions = predictor.predict(test_features)\n",
        "print(\"\\n--- First 5 Predictions ---\")\n",
        "print(predictions.head())\n",
        "\n",
        "# %%\n",
        "# @title Get Prediction Probabilities (For Classification Tasks)\n",
        "# Get the probability of each class for the test set.\n",
        "# This is useful for understanding model confidence.\n",
        "probas = predictor.predict_proba(test_features)\n",
        "print(\"\\n--- First 5 Prediction Probabilities ---\")\n",
        "print(probas.head())\n",
        "\n",
        "# %%\n",
        "# @title Extract Embeddings (Optional)\n",
        "# Extract feature embeddings for the test data.\n",
        "# These can be used for downstream tasks like clustering or custom models.\n",
        "embeddings = predictor.extract_embedding(test_features)\n",
        "print(f\"\\n--- Embedding Shape for Test Data ---\")\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Embedding for first sample (first 10 values): {embeddings[0][:10]}\")"
      ],
      "metadata": {
        "id": "1ksPoD9-sUWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# @title Evaluate Prediction Quality: Confusion Matrix and Other Metrics\n",
        "# This section demonstrates how to assess the quality of the predictions using a confusion matrix\n",
        "# and other common classification metrics.\n",
        "print(\"\\n--- Evaluating Prediction Quality ---\")\n",
        "\n",
        "# Import necessary libraries for evaluation\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Get the true labels from the test data\n",
        "true_labels = test_data[label_col]\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sorted(true_labels.unique()), yticklabels=sorted(true_labels.unique()))\n",
        "plt.title('Confusion Matrix: True vs Predicted')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Print a detailed classification report\n",
        "print(\"\\n--- Detailed Classification Report ---\")\n",
        "print(classification_report(true_labels, predictions))\n",
        "\n",
        "# Calculate and print some additional common metrics based on the confusion matrix\n",
        "tn = cm[0, 0] # True Negatives (assuming class 0 is negative)\n",
        "fp = cm[0, 1] # False Positives\n",
        "fn = cm[1, 0] # False Negatives\n",
        "tp = cm[1, 1] # True Positives (assuming class 1 is positive)\n",
        "\n",
        "accuracy_sklearn = (tp + tn) / (tp + tn + fp + fn)\n",
        "precision_sklearn = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall_sklearn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "f1_sklearn = 2 * (precision_sklearn * recall_sklearn) / (precision_sklearn + recall_sklearn) if (precision_sklearn + recall_sklearn) > 0 else 0\n",
        "\n",
        "print(f\"\\n--- Manually Calculated Metrics (for binary case, assuming class 1 is positive) ---\")\n",
        "print(f\"Accuracy (from sklearn calculation): {accuracy_sklearn:.4f}\")\n",
        "print(f\"Precision (Positive Class): {precision_sklearn:.4f}\")\n",
        "print(f\"Recall (Positive Class) / Sensitivity: {recall_sklearn:.4f}\")\n",
        "print(f\"F1-Score (Positive Class): {f1_sklearn:.4f}\")\n",
        "# Specificity\n",
        "specificity_sklearn = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "print(f\"Specificity (Negative Class): {specificity_sklearn:.4f}\")\n",
        "\n",
        "# Compare with the score obtained from the predictor.evaluate function\n",
        "print(f\"\\n--- Comparison with AutoGluon's evaluate() ---\")\n",
        "print(f\"AutoGluon Accuracy: {scores['accuracy']:.4f}\")\n",
        "# Note: AutoGluon's 'roc_auc' might differ slightly depending on the exact calculation method used.\n",
        "# The sklearn report provides Precision, Recall, F1 per class and macro/micro averages.\n",
        "print(f\"AutoGluon ROC-AUC: {scores['roc_auc']:.4f}\")"
      ],
      "metadata": {
        "id": "7h-M2b7ezIFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# @title Save and Load the Trained Model\n",
        "# Save the trained model for later use.\n",
        "import uuid\n",
        "model_path = f\"./tmp/{uuid.uuid4().hex}-saved_model\"\n",
        "os.makedirs(os.path.dirname(model_path), exist_ok=True) # Ensure directory exists\n",
        "\n",
        "predictor.save(path=model_path)\n",
        "print(f\"\\nModel saved to: {model_path}\")\n",
        "\n",
        "# Load the model back\n",
        "loaded_predictor = MultiModalPredictor.load(path=model_path)\n",
        "print(f\"Model loaded from: {model_path}\")\n",
        "\n",
        "# Verify loaded model works by evaluating again\n",
        "scores_loaded = loaded_predictor.evaluate(test_data, metrics=['roc_auc'])\n",
        "print(f\"Evaluation score of loaded model: {scores_loaded}\")\n",
        "\n",
        "# Clean up saved model directory if needed\n",
        "# import shutil\n",
        "# shutil.rmtree(model_path)\n",
        "print(\"\\nModel saving and loading example completed.\")"
      ],
      "metadata": {
        "id": "5mv2CoensbdJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}