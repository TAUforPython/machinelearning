{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAUforPython/machinelearning/blob/main/example_GNN_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "export TORCH=2.5.1\n",
        "export CUDA=cu121"
      ],
      "metadata": {
        "id": "wOpmCrUcxgT7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q --pre torch -f https://download.pytorch.org/whl/sparse/cpu/torch_sparse.html\n",
        "!pip3 install -q --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-2.5.1+cu121.html\n",
        "!pip3 install -q pyg-lib -f https://pytorch-geometric.com/whl/torch-2.5.1+cu121.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmQvglaVx7aW",
        "outputId": "7f486d2c-872d-4bce-9412-bc34811ca393"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q ogb\n",
        "!pip3 install -q torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXCZersqyje2",
        "outputId": "f21959ce-4b65-4a2a-f29f-39bd510a28a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch_geometric.nn import MessagePassing, SAGEConv\n",
        "from ogb.nodeproppred import Evaluator, PygNodePropPredDataset\n",
        "\n",
        "\n",
        "import torch_geometric.transforms as T"
      ],
      "metadata": {
        "id": "cCspREd5zXCZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph: The ogbn-arxiv dataset is a directed graph, representing the citation network between all Computer Science (CS) arXiv papers indexed by MAG [1]. Each node is an arXiv paper and each directed edge indicates that one paper cites another one. Each paper comes with a 128-dimensional feature vector obtained by averaging the embeddings of words in its title and abstract. The embeddings of individual words are computed by running the skip-gram model [2] over the MAG corpus. We also provide the mapping from MAG paper IDs into the raw texts of titles and abstracts here. In addition, all papers are also associated with the year that the corresponding paper was published."
      ],
      "metadata": {
        "id": "HM0gEoBkQwlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph: The ogbn-proteins dataset is an undirected, weighted, and typed (according to species) graph. Nodes represent proteins, and edges indicate different types of biologically meaningful associations between proteins, e.g., physical interactions, co-expression or homology [1,2]. All edges come with 8-dimensional features, where each dimension represents the approximate confidence of a single association type and takes values between 0 and 1 (the larger the value is, the more confident we are about the association). The proteins come from 8 species."
      ],
      "metadata": {
        "id": "PK2h1tgNntst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_dataset = 'ogbn-arxiv'\n",
        "\n",
        "# Это загрузит ogbn-arxiv в папку \"networks\"\n",
        "dataset = PygNodePropPredDataset(name=target_dataset, root='networks')\n",
        "\n",
        "\n",
        "#target_dataset = 'ogbn-proteins'\n",
        "#dataset = PygNodePropPredDataset(name='ogbn-proteins', transform=T.ToSparseTensor(attr='edge_attr'))\n",
        "#dataset = PygNodePropPredDataset(name=target_dataset, root='networks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OnSzcLzzccR",
        "outputId": "ac906c61-fcb7-4884-e3e7-a6e86ef4ef03"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:03<00:00, 24.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting networks/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2286.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 458.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n",
            "/usr/local/lib/python3.11/dist-packages/ogb/nodeproppred/dataset_pyg.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[0]\n",
        "#data"
      ],
      "metadata": {
        "id": "STDC--Ni0mtM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data.x: Node feature matrix with shape [num_nodes, num_node_features]\n",
        "\n",
        "data.edge_index: Graph connectivity in COO format with shape [2, num_edges] and type torch.long\n",
        "\n",
        "data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
        "\n",
        "data.y: Target to train against (may have arbitrary shape), e.g., node-level\n",
        "targets of shape [num_nodes, *] or graph-level targets of shape [1, *]\n",
        "\n",
        "data.pos: Node position matrix with shape [num_nodes, num_dimensions]\n"
      ],
      "metadata": {
        "id": "5VAy6kOZZD8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_idx = dataset.get_idx_split()\n",
        "train_idx = split_idx['train']\n",
        "valid_idx = split_idx['valid']\n",
        "test_idx = split_idx['test']"
      ],
      "metadata": {
        "id": "G3P6nxGd08Vd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = NeighborLoader(data, input_nodes=train_idx,\n",
        "                               shuffle=True, num_workers=os.cpu_count() - 2,\n",
        "                               batch_size=1024, num_neighbors=[30] * 2)\n",
        "total_loader = NeighborLoader(data, input_nodes=None, num_neighbors=[-1],\n",
        "                               batch_size=4096, shuffle=False,\n",
        "                               num_workers=os.cpu_count() - 2)"
      ],
      "metadata": {
        "id": "rvlLlJZ51C6n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels,\n",
        "                 hidden_channels, out_channels,\n",
        "                 n_layers=2):\n",
        "      super(SAGE, self).__init__()\n",
        "      self.n_layers = n_layers\n",
        "      self.layers = torch.nn.ModuleList()\n",
        "      self.layers_bn = torch.nn.ModuleList()\n",
        "      if n_layers == 1:\n",
        "        self.layers.append(SAGEConv(in_channels, out_channels,   normalize=False))\n",
        "      elif n_layers == 2:\n",
        "        self.layers.append(SAGEConv(in_channels, hidden_channels, normalize=False))\n",
        "        self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.layers.append(SAGEConv(hidden_channels, out_channels, normalize=False))\n",
        "      else:\n",
        "        self.layers.append(SAGEConv(in_channels, hidden_channels, normalize=False))\n",
        "        self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "      for _ in range(n_layers - 2):\n",
        "        self.layers.append(SAGEConv(hidden_channels,  hidden_channels, normalize=False))\n",
        "        self.layers_bn.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.layers.append(SAGEConv(hidden_channels, out_channels, normalize=False))\n",
        "      for layer in self.layers:\n",
        "        layer.reset_parameters()\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "      if len(self.layers) > 1:\n",
        "        looper = self.layers[:-1]\n",
        "      else:\n",
        "        looper = self.layers\n",
        "      for i, layer in enumerate(looper):\n",
        "        x = layer(x, edge_index)\n",
        "        try:\n",
        "          x = self.layers_bn[i](x)\n",
        "        except Exception as e:\n",
        "          abs(1)\n",
        "        finally:\n",
        "          x = F.relu(x)\n",
        "          x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "      if len(self.layers) > 1:\n",
        "        x = self.layers[-1](x, edge_index)\n",
        "\n",
        "      return F.log_softmax(x, dim=-1), torch.var(x)\n",
        "\n",
        "    def inference(self, total_loader, device):\n",
        "      xs = []\n",
        "      var_ = []\n",
        "      for batch in total_loader:\n",
        "        out, var = self.forward(batch.x.to(device), batch.edge_index.to(device))\n",
        "        out = out[:batch.batch_size]\n",
        "        xs.append(out.cpu())\n",
        "        var_.append(var.item())\n",
        "\n",
        "      out_all = torch.cat(xs, dim=0)\n",
        "      return out_all, var_"
      ],
      "metadata": {
        "id": "62YmPUWj2Blg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.num_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGfyS8I-YWrG",
        "outputId": "ca284e99-c431-436c-a99a-693fc81ce627"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = SAGE(data.num_features, 256, dataset.num_classes, n_layers=2)\n",
        "\n",
        "#model = SAGE(data.x.shape[1], 256, dataset.num_classes, n_layers=2)\n",
        "model.to(device)\n",
        "epochs = 2\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.03)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'max', patience=7)"
      ],
      "metadata": {
        "id": "G_D5ICpf5wB9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device):\n",
        "  evaluator = Evaluator(name=target_dataset)\n",
        "  model.eval()\n",
        "  out, var = model.inference(total_loader, device)\n",
        "\n",
        "  y_true = data.y.cpu()\n",
        "  y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "  train_acc = evaluator.eval({\n",
        "      'y_true': y_true[split_idx['train']],\n",
        "      'y_pred': y_pred[split_idx['train']],\n",
        "      })['acc']\n",
        "  val_acc = evaluator.eval({\n",
        "      'y_true': y_true[split_idx['valid']],\n",
        "      'y_pred': y_pred[split_idx['valid']],\n",
        "      })['acc']\n",
        "  test_acc = evaluator.eval({\n",
        "      'y_true': y_true[split_idx['test']],\n",
        "      'y_pred': y_pred[split_idx['test']],\n",
        "      })['acc']\n",
        "  return train_acc, val_acc, test_acc, torch.mean(torch.Tensor(var))"
      ],
      "metadata": {
        "id": "ODZNlfRj5zNx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction task: The task is to predict the 40 subject areas of arXiv CS papers, e.g., cs.AI, cs.LG, and cs.OS, which are manually determined (i.e., labeled) by the paper’s authors and arXiv moderators. With the volume of scientific publications doubling every 12 years over the past century, it is practically important to automatically classify each publication’s areas and topics. Formally, the task is to predict the primary categories of the arXiv papers, which is formulated as a 40-class classification problem."
      ],
      "metadata": {
        "id": "UoGN8HDkRBRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction task: The task is to predict the presence of protein functions in a multi-label binary classification setup, where there are 112 kinds of labels to predict in total. The performance is measured by the average of ROC-AUC scores across the 112 tasks."
      ],
      "metadata": {
        "id": "uOlzIy6Fp3lW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUErvtTDqdlj",
        "outputId": "43f23c51-f7b4-400b-8677-706e33f9568a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 4],\n",
            "        [ 5],\n",
            "        [28],\n",
            "        ...,\n",
            "        [10],\n",
            "        [ 4],\n",
            "        [ 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs):\n",
        "  model.train()\n",
        "\n",
        "  pbar = tqdm(total=train_idx.size(0))\n",
        "  pbar.set_description(f'Epoch {epoch:02d}')\n",
        "\n",
        "  total_loss = total_correct = 0\n",
        "\n",
        "  for batch in train_loader:\n",
        "    batch_size = batch.batch_size\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #out, _ = model(batch.x.to(device), batch.edge_index.to(device))\n",
        "    out, _ = model(batch.x, batch.edge_index)\n",
        "    out = out[:batch_size]\n",
        "\n",
        "    batch_y = batch.y[:batch_size].to(device)\n",
        "    batch_y = torch.reshape(batch_y, (-1,))\n",
        "\n",
        "    loss = F.nll_loss(out, batch_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += float(loss)\n",
        "    total_correct += int(out.argmax(dim=-1).eq(batch_y).sum())\n",
        "    pbar.update(batch.batch_size)\n",
        "  pbar.close()\n",
        "\n",
        "  loss = total_loss / len(train_loader)\n",
        "  approx_acc = total_correct / train_idx.size(0)\n",
        "\n",
        "  train_acc, val_acc, test_acc, var = test(model, device)\n",
        "\n",
        "  print(f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Var: {var:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5nIFc7G6R9s",
        "outputId": "e8b6e8ee-c9b4-4863-acb5-35b6ea788723"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 01: 100%|██████████| 90941/90941 [00:26<00:00, 3423.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 0.5690, Val: 0.5623, Test: 0.5080, Var: 8.4021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}, Var: {var:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyD6HNHFNbg8",
        "outputId": "f7493d81-566c-40e2-8128-a12119ae98a0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 0.5690, Val: 0.5623, Test: 0.5080, Var: 8.4021\n"
          ]
        }
      ]
    }
  ]
}