{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 565657,
          "sourceType": "datasetVersion",
          "datasetId": 273093
        }
      ],
      "dockerImageVersionId": 29840,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Basics of feed forward neural networks",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAUforPython/machinelearning/blob/main/Feed%20forward%20neural%20networks%20-%20example\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'uci-credit-approval-data-set:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F273093%2F565657%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240928%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240928T213928Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3bc9097e4e0e0132f2106365d9c43441b4d7a84868d941a28af67942f05a456aefaa141303d077edb28163a9e506133686995a735db9efa10f31a703455c693b38de26cff87806a1b57765ecee177b46070e71f14f9380534d2dec634b9f87e104d99aff92308f70138a845961a10dd11278c56b9edd2caeb79f8d013f06abcb68f9236307d9a0268e06446eb8a45b2b75330a1741efaeafdac2ca11c24feb732140ed6f592a7e09c983e975a7eb03c348706c7c8942f9565a5bcc765df2ce643d8ad7c252db6cda5cbd11b0710fd93dec2e04eb70304fa1e318be1f8cc3cc149b00f529b4d55e5d01604e88361fe5586fcd4beaf1e82ad01da2cc48f1e241d6'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "owYTVaX7kyw9"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports and workspace setting**"
      ],
      "metadata": {
        "id": "8nEFaXy0kyxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "b_qmMid3kyxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading data**"
      ],
      "metadata": {
        "id": "JFnQEU7TkyxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('../input/uci-credit-approval-data-set/UCI_crx.csv')\n",
        "dataset.shape"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "UoUjuUFAkyxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Zndxnj5ZkyxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting data types to tensor supported data types**"
      ],
      "metadata": {
        "id": "2OM8ND3ekyxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.dtypes"
      ],
      "metadata": {
        "trusted": true,
        "id": "3DZ5TgvXkyxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['A1', 'A2','A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13', 'A14', 'A16']:\n",
        "    dataset[col] = pd.Categorical(dataset[col])\n",
        "    dataset[col] = dataset[col].cat.codes"
      ],
      "metadata": {
        "trusted": true,
        "id": "ivSLj6CSkyxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.dtypes"
      ],
      "metadata": {
        "trusted": true,
        "id": "VecwPyvYkyxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "O_1kEd0lkyxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizing input and output vectors**\n",
        "\n",
        "Scale and normalize to make the magnitude of the features similar. Most popular methods for neumerical data are Min Max normalization and Standard normalization (z score). This will improve the model performance and convergence.\n",
        "\n",
        "https://visualstudiomagazine.com/articles/2014/01/01/how-to-standardize-data-for-neural-networks.aspx\n",
        "https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/"
      ],
      "metadata": {
        "id": "8BO39qrtkyxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create scaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "dataset = pd.DataFrame(scaler.fit_transform(dataset))\n",
        "\n",
        "dataset.describe()"
      ],
      "metadata": {
        "trusted": true,
        "id": "BD8JVtFokyxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=dataset.iloc[:,0:15].values   #0:15\n",
        "Y=dataset.iloc[:,15:16].values"
      ],
      "metadata": {
        "trusted": true,
        "id": "OOk4ULwRkyxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42) # Makes the random numbers predictable for easy comparison of models"
      ],
      "metadata": {
        "trusted": true,
        "id": "WWDvpTEGkyxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create the model generator function**\n",
        "\n",
        "Deciding neuron count in each layer and number of hidden layers is difficult and there is no straightforward answer to this.\n",
        "\n",
        "https://www.heatonresearch.com/2017/06/01/hidden-layers.html\n",
        "\n",
        "https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw"
      ],
      "metadata": {
        "id": "z-TBRid_kyxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Set these parameter before calling create_model function\n",
        "depthOfNetwork = 3\n",
        "neuronCountInEachLayer = [16, 9, 1]                                 # try different depth and width\n",
        "activationFuncEachLayer = ['sigmoid', 'relu', 'sigmoid']            # try values relu, sigmoid, talh\n",
        "lossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\n",
        "regularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n",
        "\n",
        "\n",
        "def create_model(verbose=False):\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  if verbose:\n",
        "        print('Network configuration ',neuronCountInEachLayer)\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(neuronCountInEachLayer[0], input_dim=15, activation = activationFuncEachLayer[0], kernel_regularizer=regularizerFunc)) # First Layer\n",
        "\n",
        "  for x in range(1, depthOfNetwork-1):\n",
        "      model.add(tf.keras.layers.Dense(neuronCountInEachLayer[x], activation = activationFuncEachLayer[x],kernel_regularizer=regularizerFunc))         # Second layer onwards\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(neuronCountInEachLayer[depthOfNetwork-1], activation = activationFuncEachLayer[depthOfNetwork-1]))  # Output layer\n",
        "\n",
        "  model.compile(loss = lossFunction , optimizer = 'adam' , metrics = ['accuracy'] )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "trusted": true,
        "id": "yxw8B5lQkyxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing a single model for ease of understanding**"
      ],
      "metadata": {
        "id": "MhWhjmNfkyxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depthOfNetwork = 3\n",
        "neuronCountInEachLayer = [17, 8, 1]                                 # try different depth and width\n",
        "activationFuncEachLayer = ['sigmoid', 'relu', 'sigmoid']            # try values relu, sigmoid, talh\n",
        "lossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\n",
        "regularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n",
        "\n",
        "model=create_model()\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "vnJy0h_nkyxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the evaluator function**"
      ],
      "metadata": {
        "id": "p4up6y83kyxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateTheModel(verbose=False):\n",
        "    n_split=5\n",
        "    f1_scores = []\n",
        "\n",
        "    for train_index,test_index in StratifiedKFold(n_split).split(X, Y):      # StratifiedKFold, KFold\n",
        "        x_train,x_test=X[train_index],X[test_index]\n",
        "        y_train,y_test=Y[train_index],Y[test_index]\n",
        "\n",
        "        model=create_model(verbose)\n",
        "        model.fit(x_train, y_train,epochs=100, verbose=0)\n",
        "        evaluationMetrics = model.evaluate(x_test,y_test, verbose=0)\n",
        "\n",
        "        if verbose:\n",
        "            print('Model evaluation ',evaluationMetrics)   # This returns metric values for the evaluation\n",
        "\n",
        "        y_pred = np.where(model.predict(x_test) > 0.5, 1, 0)\n",
        "        f1 = f1_score(y_test, y_pred , average=\"macro\")\n",
        "\n",
        "        if verbose:\n",
        "            print('F1 score is ', f1)\n",
        "\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    return np.mean(f1_scores)"
      ],
      "metadata": {
        "trusted": true,
        "id": "vEepO9XukyxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verbose mode of the function to see F1 scores of each fold**"
      ],
      "metadata": {
        "id": "UnXs8AyCkyxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depthOfNetwork = 2\n",
        "neuronCountInEachLayer = [2, 1]                                 # try different depth and width\n",
        "activationFuncEachLayer = ['sigmoid', 'sigmoid']            # try values relu, sigmoid, talh\n",
        "lossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\n",
        "regularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n",
        "\n",
        "evaluateTheModel(True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "FFTbcfMJkyxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment with chaning the first layer neuron count**"
      ],
      "metadata": {
        "id": "hCdWJ5gRkyxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depthOfNetwork = 2\n",
        "neuronCountInEachLayer = [15, 1]                                 # try different depth and width\n",
        "activationFuncEachLayer = ['sigmoid', 'sigmoid']            # try values relu, sigmoid, talh\n",
        "lossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\n",
        "regularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n",
        "\n",
        "for i in range (3, 20):\n",
        "    neuronCountInEachLayer = [i, 1]\n",
        "    print(\"'Node count : % 3d, Mean F1 score : % 10.5f\" %(i, evaluateTheModel()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "vt_QzEsHkyxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depthOfNetwork = 2\n",
        "neuronCountInEachLayer = [15, 1]                                 # try different depth and width\n",
        "activationFuncEachLayer = ['relu', 'sigmoid']            # try values relu, sigmoid, talh\n",
        "lossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\n",
        "regularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n",
        "\n",
        "for i in range (3, 20):\n",
        "    neuronCountInEachLayer = [i, 1]\n",
        "    print(\"'Node count : % 3d, Mean F1 score : % 10.5f\" %(i, evaluateTheModel()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "wRcHYIiukyxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depthOfNetwork = 2\n",
        "neuronCountInEachLayer = [15, 1]                                 # try different depth and width\n",
        "activationFuncEachLayer = ['tanh', 'sigmoid']            # try values relu, sigmoid, talh\n",
        "lossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\n",
        "regularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n",
        "\n",
        "for i in range (3, 20):\n",
        "    neuronCountInEachLayer = [i, 1]\n",
        "    print(\"'Node count : % 3d, Mean F1 score : % 10.5f\" %(i, evaluateTheModel()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "A5IEKz5NkyxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depthOfNetwork = 2\n",
        "neuronCountInEachLayer = [15, 1]                                 # try different depth and width\n",
        "activationFuncEachLayer = ['tanh', 'sigmoid']            # try values relu, sigmoid, talh\n",
        "lossFunction = 'mean_squared_error'                                # try values binary_crossentropy, mean_squared_error\n",
        "regularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n",
        "\n",
        "for i in range (3, 20):\n",
        "    neuronCountInEachLayer = [i, 1]\n",
        "    print(\"'Node count : % 3d, Mean F1 score : % 10.5f\" %(i, evaluateTheModel()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "DMxGTNJ8kyxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment with chaning 2nd layer neuron count while keeping width to 15 neurons**"
      ],
      "metadata": {
        "id": "uoWw11SAkyxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depthOfNetwork = 3\n",
        "neuronCountInEachLayer = [18, 9, 1]                                 # try different depth and width\n",
        "activationFuncEachLayer = ['tanh', 'tanh', 'sigmoid']            # try values relu, sigmoid, talh\n",
        "lossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\n",
        "regularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n",
        "\n",
        "for i in range (15, 16):\n",
        "    for j in range (3, 20):\n",
        "        neuronCountInEachLayer = [i, j, 1]\n",
        "        print(\"'Neurons [% 3d, % 3d], Mean F1 score : % 10.5f\" %(i, j, evaluateTheModel()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "KzoYleeQkyxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trying 3 layer setup**"
      ],
      "metadata": {
        "id": "BO17zagJkyxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depthOfNetwork = 4\n",
        "neuronCountInEachLayer = [15, 8, 5, 1]                                 # try different depth and width\n",
        "activationFuncEachLayer = ['tanh', 'tanh', 'tanh','sigmoid']            # try values relu, sigmoid, talh\n",
        "lossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\n",
        "regularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n",
        "\n",
        "print(\"'Neurons [% 3d, % 3d, % 3d], Mean F1 score : % 10.5f\" %(3, 4, 3, evaluateTheModel()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "xe5i9fBQkyxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding regularization to minimize overfitting**\n",
        "\n",
        "https://machinelearningmastery.com/how-to-reduce-generalization-error-in-deep-neural-networks-with-activity-regularization-in-keras/\n",
        "\n",
        "https://stats.stackexchange.com/questions/431898/l2-lambdas-in-keras-regularizers"
      ],
      "metadata": {
        "id": "PskfcrkskyxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network configuration selected from the above is as below**"
      ],
      "metadata": {
        "id": "hmUvCAKOkyxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depthOfNetwork = 2\n",
        "neuronCountInEachLayer = [15, 1]                                 # try different depth and width\n",
        "activationFuncEachLayer = ['tanh', 'sigmoid']            # try values relu, sigmoid, talh\n",
        "lossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\n",
        "regularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n",
        "\n",
        "print(\"'Mean F1 score : % 10.5f\" %(evaluateTheModel()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "RpzHDNz0kyxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L1 regularizer with different lambda values**"
      ],
      "metadata": {
        "id": "zkao-G6CkyxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depthOfNetwork = 2\n",
        "neuronCountInEachLayer = [15,1]                                 # try different depth and width\n",
        "activationFuncEachLayer = ['tanh', 'sigmoid']            # try values relu, sigmoid, talh\n",
        "lossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\n",
        "regularizerFunc = tf.keras.regularizers.l1(0)                       # try l1 and l2 with different lambda\n",
        "\n",
        "for i in range(-5,5):\n",
        "    regularizerFunc = tf.keras.regularizers.l1(10**i)\n",
        "    print(\"'Regularizor : l1 with lambda : % 10.5f , Mean F1 score : % 10.5f\" %(10**i, evaluateTheModel()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "DSrHMy3jkyxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L2 regularizer with different lambda values**"
      ],
      "metadata": {
        "id": "DW309IGMkyxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depthOfNetwork = 2\n",
        "neuronCountInEachLayer = [15,1]                                 # try different depth and width\n",
        "activationFuncEachLayer = ['tanh', 'sigmoid']            # try values relu, sigmoid, talh\n",
        "lossFunction = 'binary_crossentropy'                                # try values binary_crossentropy, mean_squared_error\n",
        "regularizerFunc = tf.keras.regularizers.l2(0)                       # try l1 and l2 with different lambda\n",
        "\n",
        "for i in range(-5,5):\n",
        "    regularizerFunc = tf.keras.regularizers.l2(10**i)\n",
        "    print(\"'Regularizor : l2 with lambda : % 10.5f , Mean F1 score : % 10.5f\" %(10**i, evaluateTheModel()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "d11gLxiikyxJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
